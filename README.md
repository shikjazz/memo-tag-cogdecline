# ğŸ“‹ MemoTag Cognitive Decline Detection

![Streamlit App Screenshot](docs/screenshot.png)

## ğŸš€ Overview

**MemoTag** is a Streamlitâ€‘based clinical research tool that uses **speechâ€‘pause analysis** to flag potential early signs of cognitive decline. By combining OpenAIâ€™s Whisper transcription, Librosa voiceâ€‘activity detection (VAD), and anomalyâ€‘detection scoring (DBSCAN + IsolationForest), it computes a simple 0â€“100 â€œrisk score.â€ It also generates a clinicianâ€‘ready, multiâ€‘page PDF report.

---

## âš™ï¸ Features

- **Accurate ASR** via OpenAI Whisper  
- **Speechâ€‘Pause Analysis**  
  - Total pause duration  
  - Number of pauses  
  - Pauseâ€‘length distribution histogram  
- **Prosodic Metric**: speechâ€‘rate (words/sec)  
- **Anomalyâ€‘based Scoring** (0â€“100%) with a configurable â€œhighâ€‘riskâ€ threshold  
- **Interactive Dashboard**  
  - KPI cards (Duration, Speech Rate, Risk Score)  
  - Playback + transcript  
  - AgGrid table with conditional coloring  
  - Pause vs. Risk scatterplot  
  - Expandable Detailed Analysis (Overview, Methodology, Glossary, Future Work)  
- **Downloadable Reports**  
  - CSV of raw features  
  - **4â€‘page PDF**: Overview, Methodology, Results & Figures, Glossary & Next Steps  

---

## ğŸ–¥ï¸ Demo

> **Live app:** [yourâ€‘streamlitâ€‘cloudâ€‘url]  
> ![App Screenshot](docs/screenshot.png)

---

## ğŸ“¦ Installation

1. **Clone** this repository  
   ```bash
   git clone https://github.com/yourâ€‘org/memoâ€‘tagâ€‘cogdecline.git
   cd memoâ€‘tagâ€‘cogdecline
Create & activate a Python venv

bash
Copy
Edit
python3 -m venv venv
source venv/bin/activate    # macOS/Linux
venv\Scripts\activate       # Windows
Install dependencies

bash
Copy
Edit
pip install -r requirements.txt
(Optional) If you get FFmpeg errors, install it via your package manager:

bash
Copy
Edit
# macOS (Homebrew)
brew install ffmpeg
# Ubuntu/Debian
sudo apt-get install ffmpeg
â–¶ï¸ Running Locally
bash
Copy
Edit
streamlit run app.py
Open http://localhost:8501 in your browser

Upload one or more audio files (.wav, .mp3, .m4a)

Explore the dashboard, tweak the â€œHighâ€‘risk thresholdâ€ slider, and download reports.

ğŸ”§ Configuration
Choose your Whisper model size (tiny, base, small, medium, large)

Select transcription language (e.g. en, hi, fr, es)

Adjust highâ€‘risk threshold (0â€“100%) to tune sensitivity

All controls are in the sidebar.

ğŸ“ˆ Example Workflow
Upload a spoken word list or story.

Watch the transcript appear.

See your total pause time and pause count.

View the pauseâ€‘length distribution histogram.

Check your risk score and see if it exceeds your threshold.

Download a 4â€‘page PDF report to share with clinicians.

ğŸ¤ Contributing
Fork the repo

Create a feature branch: git checkout -b feature/YourFeature

Commit your changes: git commit -m "feat: add X"

Push to your fork: git push origin feature/YourFeature

Open a Pull Request against mainâ€”include screenshots or sample audio.

ğŸ“œ License
This project is released under the MIT License.

ğŸ“ Contact
Shikhar Sharma

GitHub: @shikjazz

Email: sg7039@srmist.edu.in

Built with â¤ï¸ using Streamlit & OpenAI Whisper

yaml
Copy
Edit

---

### Next Steps

1. **Add the file**:
   ```bash
   git add README.md
   git commit -m "docs: add comprehensive README"
   git push origin main
(Optional) Create a docs/screenshot.png folder for your key screenshot, or update the path in the Markdown.
